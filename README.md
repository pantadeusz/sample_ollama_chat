# Ollama Chat Application

Author: Tadeusz PuÅºniakwski

Generated using GitHub Copilot.

Prosty interfejs czatu do lokalnego API Ollama z frontendem w czystym JavaScript i backendem w Pythonie.

## Wymagania

- Python 3.8+
- Ollama zainstalowana lokalnie i uruchomiona
- Przynajmniej jeden model pobrany w Ollama (np. `ollama pull tinyllama`)

### Przetestowane modele

NastÄ™pujÄ…ce modele zostaÅ‚y przetestowane i dziaÅ‚ajÄ… poprawnie:

```
NAME                   ID              SIZE  
smallthinker:latest    945eb1864589    3.6 GB
tinyllama:latest       2644915ede35    637 MB
```

### Instalacja

1. **Sklonuj lub pobierz projekt**

2. **Zainstaluj zaleÅ¼noÅ›ci Pythona**
```bash
pip install -r requirements.txt
```

3. **Upewnij siÄ™, Å¼e Ollama dziaÅ‚a lokalnie**
```bash
# SprawdÅº czy Ollama dziaÅ‚a
curl http://localhost:11434/api/tags

# JeÅ›li nie masz modeli, pobierz jeden z nich:
ollama pull tinyllama
# lub
ollama pull smallthinker
```

## âš™ï¸ Konfiguracja

### Podstawowa konfiguracja

1. **Skopiuj przykÅ‚adowÄ… konfiguracjÄ™:**
```bash
cp config/config.example.json config/config.json
```

2. **Edytuj plik `config/config.json`** aby skonfigurowaÄ‡ domyÅ›lny model i inne opcje:

```json
{
  "model": "llama2",
  "ollama_url": "http://localhost:11434",
  "system_prompt": "JesteÅ› pomocnym asystentem AI. Odpowiadaj zwiÄ™Åºle i rzeczowo.",
  "temperature": 0.7,
  "stream": true
}
```

### Opcje konfiguracji:

- **model**: Nazwa modelu Ollama do uÅ¼ycia (np. "llama2", "mistral", "codellama")
- **ollama_url**: URL do lokalnej instancji Ollama
- **system_prompt**: Systemowy prompt dla asystenta AI
- **temperature**: Parametr temperatury (0.0 - 1.0) - wyÅ¼sza wartoÅ›Ä‡ = bardziej kreatywne odpowiedzi
- **stream**: Czy streamowaÄ‡ odpowiedzi (true/false)
- **context_directory**: (opcjonalnie) ÅšcieÅ¼ka do katalogu z plikami kontekstu (.md)
- **context_header**: (opcjonalnie) NagÅ‚Ã³wek dodawany przed kontekstem
- **context_footer**: (opcjonalnie) Stopka dodawana po kontekÅ›cie
- **starter_message**: (opcjonalnie) WiadomoÅ›Ä‡ powitalna wyÅ›wietlana uÅ¼ytkownikowi

### Zaawansowane: Dodawanie kontekstu osobistego

Aplikacja obsÅ‚uguje automatyczne Å‚adowanie kontekstu z plikÃ³w Markdown, co pozwala utworzyÄ‡ asystenta AI ze specjalistycznÄ… wiedzÄ…:

1. **UtwÃ³rz katalog kontekstu:**
```bash
mkdir context
```

2. **Dodaj pliki .md z informacjami:**
```bash
# PrzykÅ‚ad: context/cv.md, context/projects.md, context/publications.md
```

3. **Zaktualizuj config.json:**
```json
{
  "model": "mistral:latest",
  "context_directory": "../context",
  "context_header": "--- Informacje kontekstowe ---\n\n",
  "context_footer": "\n\n--- Koniec kontekstu ---\n\n",
  "system_prompt": "JesteÅ› asystentem AI z dostÄ™pem do specjalistycznej wiedzy..."
}
```

**Uwaga:** Katalog `context/` i plik `config/config.json` sÄ… w `.gitignore` i nie bÄ™dÄ… commitowane do repozytorium. To pozwala na utrzymanie prywatnoÅ›ci osobistych informacji podczas wspÃ³Å‚dzielenia kodu.

## ğŸƒ Uruchomienie

1. **Uruchom backend Flask**
```bash
cd backend
python app.py
```

Serwer uruchomi siÄ™ na `http://localhost:5000`

2. **OtwÃ³rz przeglÄ…darkÄ™**

PrzejdÅº do `http://localhost:5000`

## ğŸ§ª Testy

Projekt zawiera testy jednostkowe i integracyjne napisane w pytest.

### Uruchomienie wszystkich testÃ³w:
```bash
pytest
```

### Uruchomienie testÃ³w z pokryciem kodu:
```bash
pytest --cov=backend --cov-report=html
```

Raport pokrycia zostanie wygenerowany w folderze `htmlcov/`.

### Uruchomienie konkretnego pliku testowego:
```bash
pytest tests/test_config_loader.py
pytest tests/test_ollama_client.py
pytest tests/test_api.py
```

## ğŸ“ Struktura projektu

```
demko/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                 # GÅ‚Ã³wna aplikacja Flask
â”‚   â”œâ”€â”€ ollama_client.py       # Klient API Ollama
â”‚   â””â”€â”€ config_loader.py       # Åadowanie konfiguracji
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html             # GÅ‚Ã³wny HTML
â”‚   â”œâ”€â”€ app.js                 # Logika czatu (czysty JS)
â”‚   â””â”€â”€ styles.css             # Style CSS
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.json            # Aktywna konfiguracja
â”‚   â””â”€â”€ config.example.json    # PrzykÅ‚adowa konfiguracja
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py            # Konfiguracja pytest
â”‚   â”œâ”€â”€ test_config_loader.py  # Testy loadera konfiguracji
â”‚   â”œâ”€â”€ test_ollama_client.py  # Testy klienta Ollama
â”‚   â””â”€â”€ test_api.py            # Testy API endpoints
â”œâ”€â”€ requirements.txt           # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ pytest.ini                 # Konfiguracja pytest
â””â”€â”€ README.md                  # Ten plik
```

## ğŸ”§ API Endpoints

### GET `/api/config`
Zwraca aktualnÄ… konfiguracjÄ™.

### GET `/api/models`
Zwraca listÄ™ dostÄ™pnych modeli Ollama.

### POST `/api/chat`
WysyÅ‚a wiadomoÅ›Ä‡ do Ollama i zwraca odpowiedÅº.

**Request body:**
```json
{
  "messages": [
    {"role": "user", "content": "CzeÅ›Ä‡!"}
  ],
  "model": "llama2",
  "stream": true
}
```

### POST `/api/reload-config`
PrzeÅ‚adowuje konfiguracjÄ™ z pliku bez restartu serwera.

## Funkcje

- Czysty JavaScript (ECMAScript) + HTML5
- Streaming odpowiedzi w czasie rzeczywistym
- WybÃ³r modelu z listy dostÄ™pnych
- Konfiguracja przez plik JSON
- PrzeÅ‚adowanie konfiguracji bez restartu
- Responsywny interfejs
- WskaÅºnik pisania
- Historia konwersacji
- ObsÅ‚uga bÅ‚Ä™dÃ³w
- Formatowanie blokÃ³w kodu
- Testy jednostkowe i integracyjne

## ğŸ› ï¸ RozwiÄ…zywanie problemÃ³w

### Ollama nie odpowiada
```bash
# SprawdÅº czy Ollama dziaÅ‚a
ollama serve

# W nowym terminalu sprawdÅº status
curl http://localhost:11434/api/tags
```

### Model nie jest dostÄ™pny
```bash
# Zobacz listÄ™ zainstalowanych modeli
ollama list

# Pobierz nowy model
ollama pull llama2
```

### BÅ‚Ä™dy CORS
Upewnij siÄ™, Å¼e `flask-cors` jest zainstalowane:
```bash
pip install flask-cors
```

### Port zajÄ™ty
ZmieÅ„ port w `backend/app.py`:
```python
app.run(debug=True, host='0.0.0.0', port=5001)  # ZmieÅ„ na inny port
```

## ğŸ“ Licencja

MIT License

Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

## ğŸ¤ WspÃ³Å‚praca

JeÅ›li chcesz rozwinÄ…Ä‡ projekt:
1. Dodaj wiÄ™cej opcji konfiguracji
2. Zaimplementuj zapisywanie historii czatu
3. Dodaj wsparcie dla wielu konwersacji
4. Dodaj eksport konwersacji do pliku
5. Zaimplementuj uwierzytelnianie uÅ¼ytkownika

## ğŸ“ Wsparcie

W razie problemÃ³w sprawdÅº:
- [Dokumentacja Ollama](https://github.com/ollama/ollama)
- [Dokumentacja Flask](https://flask.palletsprojects.com/)
- [MDN Web Docs](https://developer.mozilla.org/) dla JavaScript
